import os
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import pickle

from models import BeamNetDropout
from utils_data import (
    BeamDataset, 
    inverse_transform_predictions,
    get_dataloaders
)

# ============================================================
# üîπ Configura√ß√µes
# ============================================================
DATA_DIR = "../build/output/ml_dataset_100k"  # ‚Üê Mesmo usado no treino
MODEL_PATH = "experiments/beamnet_dropout_best.pt"  # modelo com dropout
SAVE_DIR = os.path.dirname(MODEL_PATH)
os.makedirs(SAVE_DIR, exist_ok=True)
print(f"üìÅ Salvando gr√°ficos em: {SAVE_DIR}")

device = "mps" if torch.backends.mps.is_available() else (
         "cuda" if torch.cuda.is_available() else "cpu")
print(f"üöÄ Usando dispositivo: {device}")


# ============================================================
# üîπ Carregar modelo E scaler do checkpoint
# ============================================================
print(f"\nüìÇ Carregando checkpoint de: {MODEL_PATH}")
checkpoint = torch.load(MODEL_PATH, map_location=device)

# ‚úÖ CORRIGIDO: Carregar modelo do dicion√°rio
model = BeamNetDropout(input_dim=11, hidden_dim=512, dropout_p=0.05)  # ‚Üê input_dim=11 (corrigido)
model.load_state_dict(checkpoint["model_state"])  # ‚Üê Correto
model.to(device)
model.eval()

# ‚úÖ CORRIGIDO: Carregar scaler_y do checkpoint (com pickle)
scaler_y = pickle.loads(checkpoint["scaler_y_pickle"])  # ‚Üê Mesmo scaler do treino
print(f"‚úì Modelo carregado")
print(f"‚úì Scaler_y carregado (mesmo do treino)")

# Carregar dataset apenas para scaler_x (para normaliza√ß√£o de X)
train_loader, val_loader, dataset = get_dataloaders(
    DATA_DIR,
    n_samples=None,
    batch_size=512,
    normalize=True,
    val_split=0.2
)

params = pd.read_csv(os.path.join(DATA_DIR, "parameters.csv"))


# ============================================================
# üîπ Fun√ß√£o Monte Carlo Dropout (para incerteza)
# ============================================================
def predict_with_uncertainty(model, X_tensor, n_samples=100):
    """
    ‚úÖ CORRIGIDO: Executa m√∫ltiplas predi√ß√µes com dropout ativo
    Retorna predi√ß√µes em espa√ßo TRANSFORMADO
    """
    model.train()  # Mant√©m dropout ativo
    preds = []
    
    with torch.no_grad():
        for _ in range(n_samples):
            pred = model(X_tensor)  # Predi√ß√£o em espa√ßo transformado
            preds.append(pred.cpu().numpy())
    
    preds = np.array(preds)  # [n_samples, batch_size, 1]
    
    mean_pred_transformed = preds.mean(axis=0).flatten()  # M√©dia em espa√ßo transformado
    std_pred_transformed = preds.std(axis=0).flatten()    # Std em espa√ßo transformado
    
    return mean_pred_transformed, std_pred_transformed


# ============================================================
# üîπ Fun√ß√£o para inverter incerteza
# ============================================================
def invert_uncertainty(mean_trans, std_trans, scaler_y, n_samples=100):
    """
    ‚úÖ CORRIGIDO: Inverter m√©dia e estimar incerteza em espa√ßo original
    
    Para a m√©dia: usar inverse_transform
    Para o desvio padr√£o: usar an√°lise de sensibilidade
    """
    
    # Converter m√©dia
    mean_original = inverse_transform_predictions(mean_trans, scaler_y)
    
    # Para o desvio padr√£o, usar propaga√ß√£o de incerteza
    # Aproxima√ß√£o: calcular em torno da m√©dia
    delta = std_trans * 0.01  # Pequeno desvio
    
    # Perturbar em torno da m√©dia
    lower_trans = (mean_trans - 2*std_trans).reshape(-1, 1)
    upper_trans = (mean_trans + 2*std_trans).reshape(-1, 1)
    
    lower_orig = inverse_transform_predictions(lower_trans, scaler_y).flatten()
    upper_orig = inverse_transform_predictions(upper_trans, scaler_y).flatten()
    
    # Estimar novo desvio padr√£o
    std_original = (upper_orig - lower_orig) / 4  # 4 = 2*2 (¬±2œÉ)
    
    return mean_original, std_original


# ============================================================
# üîπ Fun√ß√£o para gerar e salvar gr√°fico de incerteza
# ============================================================
def plot_uncertainty(sample_idx=0, n_mc_samples=100):
    """
    ‚úÖ CORRIGIDO: Gera gr√°fico com incerteza em espa√ßo ORIGINAL
    """
    print(f"\nüìä Gerando gr√°fico para amostra {sample_idx}...")
    
    df = pd.read_csv(os.path.join(DATA_DIR, f"sample_{sample_idx:04d}.csv"))
    E, I, q = params.loc[sample_idx, ["E", "I", "q"]]

    # Ordenar por posi√ß√£o x
    df = df.sort_values(by="x")
    x = df["x"].values

    # Carregar y verdadeiro (VEM)
    if "displacement_scaled" in df.columns:
        y_true_transformed = df["displacement_scaled"].values
    else:
        y_true_transformed = df["displacement"].values

    # ‚úÖ CORRIGIDO: Converter y verdadeiro para espa√ßo original
    y_true = inverse_transform_predictions(y_true_transformed, scaler_y).flatten()

    # ---- Montar features (AGORA COM 11 features!) ----
    self_L = 1.0
    x_scaled = x / self_L
    x2, x3, x4 = x**2, x**3, x**4
    
    EI = E * I
    inv_EI = 1.0 / EI
    q_over_EI = q / EI
    theoretical_disp = -(q * x**2 * (6*self_L**2 - 4*self_L*x + x**2)) / (24 * E * I)

    X_in = np.stack([
        np.log10(E) * np.ones_like(x),
        np.log10(I) * np.ones_like(x),
        np.log10(abs(q) + 1e-9) * np.ones_like(x),
        x_scaled, x2, x3, x4,
        EI * np.ones_like(x),
        inv_EI * np.ones_like(x),
        q_over_EI * np.ones_like(x),
        theoretical_disp
    ], axis=1)

    # ---- Normalizar X ----
    X_norm = dataset.scaler_x.transform(X_in)
    X_tensor = torch.tensor(X_norm, dtype=torch.float32).to(device)

    # ---- Predi√ß√£o com incerteza (Monte Carlo Dropout) ----
    mean_pred_trans, std_pred_trans = predict_with_uncertainty(
        model, X_tensor, n_samples=n_mc_samples
    )

    # ‚úÖ CORRIGIDO: Inverter para espa√ßo original
    mean_pred, std_pred = invert_uncertainty(
        mean_pred_trans, std_pred_trans, scaler_y
    )

    # ---- Plot ----
    plt.figure(figsize=(10, 6))
    plt.plot(x, y_true, "k-", linewidth=2.5, label="VEM (refer√™ncia)")
    plt.plot(x, mean_pred, "r--", linewidth=2, label="NN (predi√ß√£o m√©dia)")
    plt.fill_between(
        x,
        mean_pred - 2 * std_pred,
        mean_pred + 2 * std_pred,
        color="orange",
        alpha=0.3,
        label="¬±2œÉ (incerteza)"
    )
    plt.xlabel("Posi√ß√£o x (m)", fontsize=12)
    plt.ylabel("Deslocamento (m)", fontsize=12)
    plt.title(f"Monte Carlo Dropout - Amostra {sample_idx}\n(n_samples={n_mc_samples})", fontsize=13)
    plt.legend(fontsize=11)
    plt.grid(True, ls="--", alpha=0.6)
    plt.tight_layout()

    # ---- Salvar ----
    save_path = os.path.join(SAVE_DIR, f"uncertainty_sample_{sample_idx:04d}.png")
    plt.savefig(save_path, dpi=300)
    plt.close()

    print(f"   ‚úì Gr√°fico salvo: {save_path}")
    print(f"   Erro m√©dio (espa√ßo original): {np.mean(np.abs(mean_pred - y_true)):.6f} m")
    print(f"   Desvio padr√£o m√©dio: {np.mean(std_pred):.6f} m")


# ============================================================
# üîπ Rodar para m√∫ltiplas vigas
# ============================================================
print("\nüöÄ Gerando gr√°ficos de incerteza...\n")
for idx in [0, 10, 25, 50, 75]:
    try:
        plot_uncertainty(sample_idx=idx, n_mc_samples=100)
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Erro ao processar amostra {idx}: {e}")

print("\n‚úÖ Todos os gr√°ficos foram gerados!")