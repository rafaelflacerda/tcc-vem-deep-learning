
============================================================
VEM Deep Learning Training
============================================================
Data/Hora Início: 2025-11-30 09:26:10
Dataset: meshes_10_samples.npz
Experimento: meshes_10_samples_20251130_092610

HIPERPARÂMETROS:
  Arquitetura: 6 camadas [256-256-128-128-64-64]
  Batch size: 2048
  Learning rate: 0.001
  Optimizer: AdamW (weight_decay=1e-05)
  Scheduler: ReduceLROnPlateau (patience=10, factor=0.5)
  Epochs: 200
  Precision: FP16
  Device: NVIDIA GeForce RTX 4090
  Early Stopping: Ativo (patience=20)
  
============================================================


Carregando dados...

Criando modelo...
Parâmetros treináveis: 129,794
Compilando modelo (torch.compile)...

================================================================================
Epoch   Train Loss     Val Loss       LR          Time(s)   Best 
================================================================================
1       5899.100857    2310.790100    0.001000    2.0       *    
2       5712.596137    2177.538452    0.001000    0.6       *    
3       5720.152289    2101.067627    0.001000    0.6       *    
4       5498.566678    2039.707275    0.001000    0.6       *    
5       5445.835341    1990.753906    0.001000    0.6       *    
6       5411.225043    1947.371765    0.001000    0.6       *    
7       5296.840875    1907.184998    0.001000    0.6       *    
8       5211.653483    1868.653870    0.001000    0.6       *    
9       5181.724718    1831.880310    0.001000    0.6       *    
10      5112.775608    1796.355286    0.001000    0.6       *    
11      5068.791450    1761.609558    0.001000    0.6       *    
12      5099.418783    1728.026855    0.001000    0.6       *    
13      5074.761719    1695.227112    0.001000    0.6       *    
14      5038.223958    1663.632507    0.001000    0.6       *    
15      5080.991319    1632.374695    0.001000    0.6       *    
16      4990.381565    1602.193298    0.001000    0.6       *    
17      4857.183105    1572.682251    0.001000    0.6       *    
18      4848.600315    1543.841614    0.001000    0.6       *    
19      4851.590332    1516.107727    0.001000    0.6       *    
20      4808.103299    1488.725037    0.001000    0.6       *    
21      4783.636664    1461.665955    0.001000    0.6       *    
22      4660.640245    1436.205994    0.001000    0.6       *    
23      4611.845595    1410.747925    0.001000    0.6       *    
24      4664.535645    1385.955017    0.001000    0.6       *    
25      4563.305094    1361.775757    0.001000    0.6       *    
26      4578.524306    1338.220215    0.001000    0.6       *    
27      4537.974935    1315.283997    0.001000    0.6       *    
28      4436.834364    1292.941956    0.001000    0.6       *    
29      4563.893717    1271.735504    0.001000    0.6       *    
30      4497.343370    1250.550934    0.001000    0.6       *    
31      4433.440430    1229.927917    0.001000    0.6       *    
32      4481.671305    1209.867554    0.001000    0.6       *    
33      4311.714681    1190.341461    0.001000    0.6       *    
34      4332.954427    1171.329681    0.001000    0.6       *    
35      4468.005452    1152.895538    0.001000    0.6       *    
36      4362.345242    1135.400208    0.001000    0.6       *    
37      4229.838949    1117.934204    0.001000    0.6       *    
38      4211.508979    1100.992249    0.001000    0.6       *    
39      4285.281359    1084.508453    0.001000    0.6       *    
40      4192.569770    1068.394348    0.001000    0.6       *    
41      4199.452691    1052.846069    0.001000    0.6       *    
42      4081.732259    1037.634460    0.001000    0.6       *    
43      4095.146946    1022.815186    0.001000    0.6       *    
44      4134.333984    1008.556458    0.001000    0.6       *    
45      4076.762288    995.079437     0.001000    0.6       *    
46      4103.090685    981.561066     0.001000    0.6       *    
47      4131.554470    968.502289     0.001000    0.6       *    
48      4045.069499    955.311462     0.001000    0.6       *    
49      4041.084880    943.190308     0.001000    0.6       *    
50      3979.066569    931.484222     0.001000    0.6       *    
51      3999.313666    919.507721     0.001000    0.6       *    
52      3907.491672    908.399536     0.001000    0.6       *    
53      3982.447401    897.044342     0.001000    0.6       *    
54      3943.046848    886.623474     0.001000    0.6       *    
55      3981.391547    875.987366     0.001000    0.6       *    
56      3969.618300    866.234650     0.001000    0.6       *    
57      3785.890272    856.869720     0.001000    0.6       *    
58      3821.757297    847.053101     0.001000    0.6       *    
59      3944.722873    838.184448     0.001000    0.6       *    
60      3836.956082    829.544708     0.001000    0.6       *    
61      3752.902127    820.781494     0.001000    0.6       *    
62      3804.182509    812.600952     0.001000    0.6       *    
63      3758.967909    804.631653     0.001000    0.6       *    
64      3791.435438    796.429810     0.001000    0.6       *    
65      3867.724094    789.200165     0.001000    0.7       *    
66      3851.817600    782.489380     0.001000    0.6       *    
67      3831.643717    775.983887     0.001000    0.6       *    
68      3672.961046    768.835419     0.001000    0.6       *    
69      3739.724311    762.423065     0.001000    0.6       *    
70      3730.753391    756.203827     0.001000    0.6       *    
71      3747.095486    750.498047     0.001000    0.6       *    
72      3637.431993    745.307129     0.001000    0.6       *    
73      3750.050917    739.985382     0.001000    0.6       *    
74      3700.260064    735.412415     0.001000    0.6       *    
75      3658.071343    731.302826     0.001000    0.6       *    
76      3580.832655    727.384155     0.001000    0.6       *    
77      3641.224175    723.164062     0.001000    0.6       *    
78      3708.067925    719.786865     0.001000    0.6       *    
79      3648.563287    716.598846     0.001000    0.6       *    
80      3616.561659    714.090790     0.001000    0.6       *    
81      3600.737847    712.761597     0.001000    0.6       *    
82      3616.107503    710.631226     0.001000    0.6       *    
83      3701.717312    708.522491     0.001000    0.6       *    
84      3589.463026    706.932068     0.001000    0.6       *    
85      3585.887017    705.694427     0.001000    0.6       *    
86      3587.021240    704.019531     0.001000    0.6       *    
87      3579.402534    702.024628     0.001000    0.6       *    
88      3532.905056    702.294434     0.001000    0.6            
89      3696.381456    702.546448     0.001000    0.6            
90      3634.990967    703.930450     0.001000    0.6            
91      3593.743598    706.023621     0.001000    0.6            
92      3595.646810    708.736267     0.001000    0.6            
93      3593.137994    708.438843     0.001000    0.6            
94      3599.476318    710.866150     0.001000    0.6            
95      3556.668837    713.137787     0.001000    0.6            
96      3584.133030    713.775787     0.001000    0.6            
97      3551.261176    714.330475     0.001000    0.6            
98      3533.264594    713.750458     0.000500    0.6            
99      3648.165120    714.837494     0.000500    0.6            
100     3620.402317    716.267181     0.000500    0.6            
101     3578.265001    717.896667     0.000500    0.6            
102     3598.233534    717.368011     0.000500    0.6            
103     3592.043023    716.520569     0.000500    0.6            
104     3585.976128    715.858002     0.000500    0.6            
105     3515.947917    715.846313     0.000500    0.6            
106     3570.910862    716.984161     0.000500    0.6            
107     3643.236382    716.820251     0.000500    0.6            

================================================================================
Early stopping! Sem melhora por 20 epochs.
Melhor val_loss: 702.024628 (epoch 87)

================================================================================
Treino Finalizado: 2025-11-30 09:27:22
Duração Total: 0h 1min 9s
Melhor val_loss: 702.024628
Modelo salvo em: /workspace/tcc-vem-deep-learning/models/meshes_10_samples_20251130_092610/best_model.pth
================================================================================


✓ Experimento completo: meshes_10_samples_20251130_092610
